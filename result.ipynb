{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a21e31d016d3c11b",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-08T18:34:14.891228100Z",
     "start_time": "2025-02-08T18:34:14.880560Z"
    }
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torchvision as tv\n",
    "from torch import nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dataset Omniglot\n    Number of datapoints: 18480\n    Root location: ./data/raw\\omniglot-py"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv.datasets.Omniglot(root=\"./data/raw\", background=True, download=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-08T18:34:17.092830400Z",
     "start_time": "2025-02-08T18:34:14.885711Z"
    }
   },
   "id": "50cbfc1c74a77b9"
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "class FewShotDataset(Dataset):\n",
    "  \"\"\" A custom Dataset class for Few-Shot Learning tasks.\n",
    "    This dataset can operate in two modes: \"support\" (for prototype calculation) and \"query\" (for evaluation). \"\"\"\n",
    "  def __init__(self, dataset, indices, transform, mode=\"support\"):\n",
    "    \"\"\" Args:\n",
    "        dataset (list): List of (feature, label) pairs.\n",
    "        indices (list): List of indices to be used for the dataset.\n",
    "        transform (callable): Transform to be applied to the features.\n",
    "        mode (str): Mode of operation, either \"support\" or \"query\". Default is \"support\". \"\"\"\n",
    "    self.dataset, self.indices = [] if not dataset else dataset, indices # Initialize dataset (empty if not provided)\n",
    "    self.mode, self.transform = mode, transform\n",
    "    self.classes = dataset.classes\n",
    "  # __init__():\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    if index >= len(self.indices): raise IndexError(\"Index out of bounds\")\n",
    "    feature, label = self.dataset[self.indices[index]]\n",
    "    if self.mode == \"query\":\n",
    "      one_hot_vector = torch.zeros(len(self.classes))\n",
    "      one_hot_vector[label] = 1.\n",
    "      label = one_hot_vector.requires_grad_(True)\n",
    "    return self.transform(feature), label\n",
    "  # __getitem__():\n",
    "\n",
    "  def __len__(self): return len(self.indices)\n",
    "# FSLDataset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-08T18:34:17.092830400Z",
     "start_time": "2025-02-08T18:34:17.088713300Z"
    }
   },
   "id": "edee18c2caf79dc8"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "class FewShotEpisoder:\n",
    "  \"\"\" A class to generate episodes for Few-Shot Learning.\n",
    "  Each episode consists of a support set and a query set. \"\"\"\n",
    "  def __init__(self, dataset: tv.datasets.ImageFolder, k_shot: int, n_query: int, transform: typing.Callable):\n",
    "    \"\"\" Args:\n",
    "        dataset (Dataset): The base dataset to generate episodes from.\n",
    "        k_shot (int): Number of support samples per class.\n",
    "        n_query (int): Number of query samples per class.\n",
    "        transform (callable): Transform to be applied to the features. \"\"\"\n",
    "    self.k_shot, self.n_query = k_shot, n_query  # define n-way/k-hot framework parameters\n",
    "    self.dataset, self.transform = dataset, transform  # init dataset and apply transformer\n",
    "    self.indices_c = self.get_indices()\n",
    "  # __init__()\n",
    "\n",
    "  def get_indices(self):\n",
    "    \"\"\" Initialize the class indices for the dataset.\n",
    "        * Returns: tuple of Number of classes and a list of indices grouped by class. \"\"\"\n",
    "    indices_c = [[] for _ in range(len(self.dataset.classes))]\n",
    "    for index, (feature, label) in enumerate(self.dataset): indices_c[label].append(index)\n",
    "    return indices_c\n",
    "  # get_indices():\n",
    "\n",
    "  def get_episode(self):  # select classes using list of chosen indexes\n",
    "    \"\"\" Generate an episode consisting of a support set and a query set.\n",
    "        Returns: tuple of A FewShotDataset for the support set and a FewShotDataset for the query set. \"\"\"\n",
    "    buffer_indices_c = self.indices_c.copy()\n",
    "    support_examples, query_examples = [], []\n",
    "    # select support examples\n",
    "    for index_indices, indices in enumerate(buffer_indices_c):\n",
    "      for index, x_index in enumerate(indices):\n",
    "        support_examples.append(x_index)\n",
    "        buffer_indices_c[index_indices].pop(index)\n",
    "    # select query examples\n",
    "    query_examples = support_examples.copy()\n",
    "    for index_indices, indices in enumerate(buffer_indices_c):\n",
    "      for index, x_index in enumerate(indices):\n",
    "        support_examples.append(x_index)\n",
    "        buffer_indices_c[index_indices].pop(index)\n",
    "    # init datasets\n",
    "    support_set = FewShotDataset(self.dataset, support_examples, mode=\"support\", transform=self.transform)\n",
    "    query_set = FewShotDataset(self.dataset, query_examples, mode=\"query\", transform=self.transform)\n",
    "\n",
    "    return support_set, query_set\n",
    "  # get_episode()\n",
    "# Episoder()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-08T18:34:17.133803900Z",
     "start_time": "2025-02-08T18:34:17.091583100Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "class ProtoNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ProtoNet, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.softmax = nn.LogSoftmax(dim=0)\n",
    "  # __init__():\n",
    "\n",
    "  def prototyping(self, prototypes): self.prototypes = prototypes\n",
    "\n",
    "  def cdist(self, x):\n",
    "    dists = torch.cdist(x, self.prototypes, p=2).squeeze(0)  # Efficient batch-wise L2 distance computation\n",
    "    return dists\n",
    "  # cdist()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.cdist(x)\n",
    "    x = self.softmax(x)\n",
    "    return x\n",
    "  # forward()\n",
    "# ProtoNet()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-08T18:43:32.176496700Z",
     "start_time": "2025-02-08T18:43:32.168888200Z"
    }
   },
   "id": "c79da1b370080d15"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs/episodes:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\titerations/queries:   0%|          | 0/5 [00:00<?, ?it/s]\u001B[A\n",
      "\titerations/queries:  20%|██        | 1/5 [00:06<00:26,  6.56s/it]\u001B[A\n",
      "\titerations/queries:  40%|████      | 2/5 [00:12<00:18,  6.33s/it]\u001B[A\n",
      "\titerations/queries:  60%|██████    | 3/5 [00:18<00:12,  6.24s/it]\u001B[A\n",
      "\titerations/queries:  80%|████████  | 4/5 [00:24<00:06,  6.20s/it]\u001B[A\n",
      "\titerations/queries: 100%|██████████| 5/5 [00:31<00:00,  6.22s/it]\u001B[A\n",
      "epochs/episodes: 100%|██████████| 1/1 [00:31<00:00, 31.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.000082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # init device\n",
    "\n",
    "# create FSL episode generator\n",
    "transform = tv.transforms.Compose([\n",
    "  tv.transforms.Resize((224, 224)),\n",
    "  tv.transforms.ToTensor(),\n",
    "  tv.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "  ]) # transform\n",
    "imageset = tv.datasets.ImageFolder(root=\"./data/raw/omniglot-py/images_background/Futurama\")\n",
    "\n",
    "# init episoder\n",
    "k_shot, n_query, iters, epochs = 5, 2, 5, 1\n",
    "episoder = FewShotEpisoder(imageset, k_shot, n_query, transform)\n",
    "\n",
    "# init learning\n",
    "model = ProtoNet().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# start episodes\n",
    "loss = float()\n",
    "for _ in tqdm(range(epochs), desc=\"epochs/episodes\"):\n",
    "  support_set, query_set = episoder.get_episode() # create support set and query set\n",
    "  # compute prototype from support examples\n",
    "  prototypes = list()\n",
    "  embedded_features_list = [[] for _ in range(len(support_set.classes))]\n",
    "  for embedded_feature, label in support_set: embedded_features_list[label].append(embedded_feature)\n",
    "  for embedded_features in embedded_features_list:\n",
    "    sum = torch.zeros_like(embedded_features[0])\n",
    "    for embedded_feature in embedded_features: sum += embedded_feature\n",
    "    sum /= len(embedded_features)\n",
    "    prototypes.append(sum.flatten())\n",
    "  prototypes = torch.stack(prototypes)\n",
    "  model.prototyping(prototypes)\n",
    "  # update loss for given iters\n",
    "  for _ in tqdm(range(iters), desc=\"\\titerations/queries\"):\n",
    "    for feature, label in DataLoader(query_set, shuffle=True):\n",
    "      loss = criterion(model.forward(feature), label.squeeze(dim=0))\n",
    "      optim.zero_grad()\n",
    "      loss.backward()\n",
    "      optim.step()\n",
    "print(f\"loss: {loss:.6f}\") # print final value of loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-08T18:44:04.618335800Z",
     "start_time": "2025-02-08T18:43:32.813612100Z"
    }
   },
   "id": "8a5cf41f755ffb9b"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.72(188/260)\n"
     ]
    }
   ],
   "source": [
    "eval_episoder = FewShotEpisoder(imageset, 4, 4, transform)\n",
    "eval_support_set, eval_query_set = eval_episoder.get_episode()\n",
    "correct, n_problem = 0, len(eval_query_set)\n",
    "# compute prototype from support examples\n",
    "prototypes = list()\n",
    "embedded_features_list = [[] for _ in range(len(support_set.classes))]\n",
    "for embedded_feature, label in support_set: embedded_features_list[label].append(embedded_feature)\n",
    "for embedded_features in embedded_features_list:\n",
    "  sum = torch.zeros_like(embedded_features[0])\n",
    "  for embedded_feature in embedded_features: sum += embedded_feature\n",
    "  sum /= len(embedded_features)\n",
    "  prototypes.append(sum.flatten())\n",
    "eval_prototypes = torch.stack(prototypes)\n",
    "model.prototyping(eval_prototypes)\n",
    "for feature, label in DataLoader(eval_query_set, shuffle=True):\n",
    "  if torch.argmax(model.forward(feature)) == torch.argmax(label): correct += 1\n",
    "print(f\"accuracy: {correct/n_problem:.2f}({correct}/{n_problem})\") # print final accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-08T18:44:07.807573600Z",
     "start_time": "2025-02-08T18:44:04.623665900Z"
    }
   },
   "id": "2c033a9c2e3d1352"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
